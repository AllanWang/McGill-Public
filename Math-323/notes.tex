\documentclass[12pt]{article}
	\usepackage{hyperref, fancyhdr, setspace, enumerate}
	\usepackage{tabulary}
	\usepackage{amsmath, amsthm, amssymb, array, keycommand, lastpage, amssymb, xcolor, mathtools}
	\usepackage{multiaudience}
	\usepackage{makecell}
	\usepackage{enumitem}
	\usepackage[margin=1 in]{geometry}
	\allowdisplaybreaks
	\hypersetup{
		%colorlinks=true, %set true if you want colored links
		linktoc=all, %set to all if you want both sections and subsections linked
		linkcolor=black, %choose some color if you want links to stand out
	}
	\author{Allan Wang} 
	\date{Last updated: \today}
	\title{MATH 323: Probability}
	\pagestyle{fancy}
	\lhead{MATH 323}
	\chead{\leftmark}
	\rhead{Allan Wang}
	\cfoot{Page \thepage \ of \pageref{LastPage}}
	
	% Only number for sections
	\setcounter{secnumdepth}{1}
	
	\newcommand\m[1]{\begin{bmatrix}#1\end{bmatrix}}
	\newcommand\mm[1]{\begin{pmatrix}#1\end{pmatrix}}

	\setlength{\parindent}{0pt}
	
	\SetNewAudience{notes}
	\SetNewAudience{full}
	
	\setlist[enumerate]{itemsep=0mm}
	\setlist[itemize]{itemsep=0mm}

	\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
	
	\newcommand{\comment}[1]{}

	\newcommand{\mathcomment}[0]{\quad\color{blue}}

	\newcommand{\bigsum}[2]{\sum\limits_{#1}^{#2}}

	\newcommand{\ddef}[1]{\textcolor{blue}{#1}}
	
	\newcommand{\real}[0]{\mathbb{R}}
	
	\newcommand{\uu}[1]{\underbracket{#1}}

	\newkeycommand{\ccup}[sub=i=1, sup=\infty, base=A_i] {
		\bigcup_{\commandkey{sub}}^{\commandkey{sup}}\commandkey{base}
	}

	\newkeycommand{\ccap}[sub=i=1, sup=\infty, base=A_i] {
		\bigcap_{\commandkey{sub}}^{\commandkey{sup}}\commandkey{base}
	}

	\newkeycommand{\llim}[sub=n \rightarrow \infty, base=A_n] {
		\lim_{\commandkey{sub}}\commandkey{base}
	}

	\newkeycommand{\ssum}[sub=i=1, sup=k] {
		\sum_{\commandkey{sub}}^{\commandkey{sup}}	
	}

	\newenvironment{block}[1][Label]{\underline{#1}\par}{}
%	\newenvironment{proof}{\block[Proof]}{\endblock}
	\newenvironment{proposition}{\block[Proposition]}{\endblock}
	\newenvironment{lemma}{\block[Lemma]}{\endblock}
%	\newenvironment{theorem}{\block[Theorem]}{\endblock}
	\newenvironment{remark}{\block[Remark]}{\endblock}
	\newenvironment{definition}{\block[Definition]}{\endblock}

	\newcommand{\bb}[1]{\left\{#1\right\}}
	\newcommand{\bbb}[1]{\left[#1\right]}
	\newcommand{\pp}[1]{\left(#1\right)}
	\newcommand{\abs}[1]{\left|#1\right|}

	\newcommand{\divider}[0]{\par\textcolor{lightgray}{\rule{\textwidth}{0.1pt}}}
	
	\newenvironment{claim}{\textit{Claim:}}{\hfill $\square$}
	
	\newenvironment{remarks}{\underline{Remarks}\par}{}
	
	\newenvironment{example}{\shownto{-,notes}\underline{Example}\par}{\par\divider\endshownto}
	
	\newenvironment{eqn}{\equation\alignedat{3}}{\endalignedat\endequation}
	
	\newcommand{\todo}[0]{\textcolor{red}{\textbackslash\textbackslash TODO \ }}
	
	\newcounter{theorem}
	\newcommand{\theorem}[1]{\refstepcounter{theorem}\par\medskip
		\underline{Theorem~\thetheorem. #1}}
	
	\let\oldperp\perp
	\renewcommand{\perp}[0]{\oldperp\!\!\!\oldperp}

\begin{document}
\onehalfspacing
\maketitle
\tableofcontents
\pagebreak

\section{Discrete Random Variables}


\ddef{A random variable is discrete if it only has a finite or countably infinite number of distinct values}

\subsection{3.2-3 Discrete Probability Distribution}

\begin{itemize}
	\item $0 \le p(y) \le 1 \quad \forall y$
	\item $\sum_y p(y) = 1$
	\item $E(Y) = \sum_y yp(y)$
	\item $V(Y) = E\bbb{(Y - \mu)^2} = E(Y^2) - \mu^2$
	\item $E(c) = \sum_y c p(y) = c \sum_y p(y)$
	\item $E\bbb{cg(Y)} = cE\bbb{g(Y)}$
\end{itemize}

\subsection{3.4 Binomial Probability Distribution}

\ddef{Consists of n identical independent trials, resulting in either success ($p$) or failure ($q$). Goal is to find number of successes in $k$ trials for some $k$.}

\begin{itemize}
	\item $p(y) = \mm{n \\ y} p^y q^{n - y}$
	\item $E(Y) = np$
	\item $\sigma^2 = V(Y) = npq$
\end{itemize}

\subsection{3.5 Geometric Probability Distribution}

\ddef{Same constraints as binomial probability distribution, but we are interested in finding the odds that $Y$ is the index of the first success.}

\begin{itemize}
	\item $p(y) = q^{y - 1}p$
	\item $\mu = E(Y) = \frac{1}{p}$
	\item $\sigma^2 = V(Y) = \frac{1 - p}{p^2}$
\end{itemize}

\subsection{3.6 Negative Binomial Probability Distribution}

\ddef{Same constraints as binomial probability distribution, but we are interested in finding the position for the $k^{th}$ success, for some $k$}

\begin{itemize}
	\item $p(y) = \mm{y - 1 \\ r - 1}p^rq^{y-r}$
	\item $\mu = E(Y) = \frac{r}{p}$
	\item $\sigma^2 = V(Y) = \frac{r(1 - p)}{p^2}$
\end{itemize}

\subsection{3.8 Poisson Probability Distribution}

\ddef{Used to express probability that a certain number of events occur at a fixed time interval, given $\lambda$ is the average, granted that the events are independent and identical}

\begin{itemize}
	\item $p(y) = \frac{\lambda^y}{y!}e^{-\lambda}$
	\item $\mu = E(Y) = \lambda$
	\item $\sigma^2 = V(Y) = \lambda$
\end{itemize}

\subsection{3.9 Moment \& Moment-Generating Functions}

\begin{itemize}
	\item ${\mu'}_k = E(Y^k)$
	\item $\mu_k = E\bbb{(Y - \mu)^k}$
	\item $m(t) = E(e^{tY})$
	\item $\frac{d^k m(t)}{dt^k}\big|_{t=0} = m^{(k)}(0) = {\mu'}_k$
\end{itemize}	

\subsection{3.11 Tchebysheff's Theorem}

If $\mu < \infty$ and $\sigma^2 < \infty$, then $\forall k > 0$: \\
$P(\abs{Y - \mu} < k\sigma) \ge 1 - \frac{1}{k^2}$ or \\
$P(\abs{Y - \mu} \ge k\sigma) \le \frac{1}{k^2}$

\subsection{Chernoff Inequality}

Given a random variable $x$, moment generating function $M_x(t)$, and $c > 0$:

\begin{enumerate}
	\item $P(X \ge c) \le e^{-tc} \cdot M_x(t) \quad \forall t > 0$
	\item $P(X \le c) \le e^{-tc} \cdot M_x(t) \quad \forall t < 0$
\end{enumerate}

\subsection{Jenson's Inequality}

If $f(x)$ is a convex function, for $0 \le a \le 1 \ \forall x_1, x_2 \le \text{Domain of } f$:\\
$a \cdot f(x_1) + (1 - a) \cdot f(x_2) \ge f(ax_1 + (1 - a) \cdot x_2)$ \\

If $E(f(x)) < \infty$, then $E(f(x)) \ge f(E(x))$

\subsection{Markov Inequality}

If $E(X) < \infty$, then $P(X \ge c) \le \frac{E(X)}{c} \ \forall c > 0$

\subsection{Bernoulli Trial}

Random experiment for which, given $0 < p < 1$, $P(Y = 1) = p$, $P(Y = 0) = q = 1 - p$

\begin{enumerate}
	\item Random variable: $x$
	\item Realization: $x = 1$ (success), $x = 0$ (failure)
	\item Probability Mass Function: $p_x(x) = p^x (1-p)^{1-x} = p^x q^{1-x} \quad x = 0, 1$
	\item Moment Generating Function: $M_x(t) = E(e^{tx}) = \sum_{x=0}^1 e^{tx} \cdot p^x \cdot (1 - p)^{1-x} = q + p \cdot e^t$
	\item Expectation $= p$, Variance $=pq$
\end{enumerate}

\section{Final Review}

\subsection{Inequality}

\begin{itemize}
	\item Kolmogorov's Axioms \\
	Last point: If $A_i \cap A_j = \emptyset$, then $P(A_i \cup A_j) = P(A_i) + P(A_j)$
	\item Boole's Inequality \\
	For $A_1, A_2, ... \in F$, $P\pp{\bigcup_{i = 1}^\infty A_i} \le \sum_{i = 1}^\infty P(A_i)$ \\
	Prove using disjoint settings, using axiom above for summation, and showing that an intersection $A \cap B$ is always less than or equal to $A$.
	\item Bonferroni's Inequality \\
	For $A_1, A_2, ... \in F$, $P\pp{\bigcap_{i = 1}^\infty A_i} \ge 1 - \sum_{i = 1}^\infty P(A_i^C)$ \\
	Prove using DeMorgan's, then Boole's to form the inequality.
\end{itemize}

\subsection{Independence}

	Prove that $A$ is independent from $B$ iff $A^C$ is independent from $B$

\subsection{Moment Generation Function}

Used to calculate moments (expectation and variance) and showing that distributions are equal.

To find the expectation, derive MGF and set $t = 0$, or derive $log(MGF)$ and set $t = 1$

An MGF is composed of the normalizing constant and the kernel

\subsection{Transformation}

\begin{itemize}
	\item Jacobian
	\item CDF
	
	\begin{eqn}
		P(Y \le y) &= P(g(X) \le y) \\ 
		&= P(X \le g^{-1}(y)) \\
		&= P_x(g^{-1}(y)) \\\\
		f_y(y) &= F'_y(y) \\
		&= F'_x(f^{-1}(y)) \frac{d}{dy} g^{-1}(y)
	\end{eqn}
\end{itemize}

\end{document}