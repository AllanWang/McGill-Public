\documentclass[12pt]{article}
	\usepackage{hyperref, fancyhdr, setspace, enumerate}
	\usepackage{tabulary}
	\usepackage{amsmath, amsthm, amssymb, array, keycommand, lastpage, amssymb, xcolor, mathtools}
	\usepackage{multiaudience}
	\usepackage{tabularx}
	\usepackage{makecell}
	\usepackage{enumitem}
	\usepackage[margin=1 in]{geometry}
	\allowdisplaybreaks
	\hypersetup{
		%colorlinks=true, %set true if you want colored links
		linktoc=all, %set to all if you want both sections and subsections linked
		linkcolor=black, %choose some color if you want links to stand out
	}
	\author{Allan Wang} 
	\date{Last updated: \today}
	\title{Comp 551: Machine Learning}
	\pagestyle{fancy}
	\lhead{COMP 551}
	\chead{\leftmark}
	\rhead{Allan Wang}
	\cfoot{Page \thepage \ of \pageref{LastPage}}
	
	% Only number for sections
	\setcounter{secnumdepth}{1}
	
	\newcommand\mm[1]{\begin{pmatrix}#1\end{pmatrix}}

	\setlength{\parindent}{0pt}
	
	\SetNewAudience{notes}
	\SetNewAudience{full}
	
	\setlist[enumerate]{itemsep=0m	m}
	\setlist[itemize]{itemsep=0mm}

	\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
	
	\newcommand{\comment}[1]{}

	\newcommand{\mathcomment}[0]{\quad\color{blue}}

	\newcommand{\bigsum}[2]{\sum\limits_{#1}^{#2}}

	\newcommand{\ddef}[1]{\textcolor{blue}{#1}}
	
	\newcommand{\real}[0]{\mathbb{R}}
	
	\newcommand{\uu}[1]{\underbracket{#1}}

	\newkeycommand{\ccup}[sub=i=1, sup=\infty, base=A_i] {
		\bigcup_{\commandkey{sub}}^{\commandkey{sup}}\commandkey{base}
	}

	\newkeycommand{\ccap}[sub=i=1, sup=\infty, base=A_i] {
		\bigcap_{\commandkey{sub}}^{\commandkey{sup}}\commandkey{base}
	}

	\newkeycommand{\llim}[sub=n \rightarrow \infty, base=A_n] {
		\lim_{\commandkey{sub}}\commandkey{base}
	}

	\newkeycommand{\ssum}[sub=i=1, sup=k] {
		\sum_{\commandkey{sub}}^{\commandkey{sup}}	
	}

	\newenvironment{block}[1][Label]{\underline{#1}\par}{}
%	\newenvironment{proof}{\block[Proof]}{\endblock}
	\newenvironment{proposition}{\block[Proposition]}{\endblock}
	\newenvironment{lemma}{\block[Lemma]}{\endblock}
%	\newenvironment{theorem}{\block[Theorem]}{\endblock}
	\newenvironment{remark}{\block[Remark]}{\endblock}
	\newenvironment{definition}{\block[Definition]}{\endblock}

	\newcommand{\bb}[1]{\left\{#1\right\}}
	\newcommand{\bbb}[1]{\left[#1\right]}
	\newcommand{\pp}[1]{\left(#1\right)}
	\newcommand{\abs}[1]{\left|#1\right|}

	\newcommand{\divider}[0]{\par\textcolor{lightgray}{\rule{\textwidth}{0.1pt}}}
	
	\newenvironment{claim}{\textit{Claim:}}{\hfill $\square$}
	
	\newenvironment{remarks}{\underline{Remarks}\par}{}
	
	\newenvironment{example}{\shownto{-,notes}\underline{Example}\par}{\par\divider\endshownto}
	
	\newenvironment{eqn}{\equation\alignedat{3}}{\endalignedat\endequation}
	
	\newcommand{\todo}[0]{\textcolor{red}{\textbackslash\textbackslash TODO \ }}
	
	\newcounter{theorem}
	\newcommand{\theorem}[1]{\refstepcounter{theorem}\par\medskip
		\underline{Theorem~\thetheorem. #1}}
	
	\let\oldperp\perp
	\renewcommand{\perp}[0]{\oldperp\!\!\!\oldperp}

\begin{document}
\onehalfspacing
\maketitle
\tableofcontents
\pagebreak

\section{Linear Regression}

\begin{itemize}
	\item Classification - discrete set output
	\item Regression - continuous output
	\item Supervised learning - given training examples with labels, find model to predict labels given input
	\item i.i.d assumption - training set is assumed to be \textit{independently} and \textit{identically distributed}
	\item Linear hypothesis - find weights to minimize
	\begin{eqn}
		Err() = \Sigma_{i = 1:n} (y_i - w^T x_i)^2
	\end{eqn}
\end{itemize}

\subsection{Least-Squares Error}

\begin{eqn}
	f_w &= argmin \Sigma_{i=1:n}(y_i - w^T x_i)^2 \\
	\hat{w} &= (X^T X)^{-1} X^T Y
\end{eqn}
\begin{itemize}
	\item Note that both w and x vectors have an extra dimension (m + 1) for a dummy/intercept term (all 1s for x)
	\item Computational cost is $O(m^3 + nm^2)$ for n inputs and m features
	\item Only works if $X^T X$ is nonsingular (no correlation between features)
	\begin{itemize}
		\item If a feature is a linear function of others, you can drop it
		\item If the number of features exceeds number of training examples, you can reduce the number of features
	\end{itemize}
\end{itemize}

\subsection{Gradient Descent}

\begin{eqn}
	w_{k + 1} &= w_k - \alpha_k \frac{\partial Err(w_k)}{\partial w_k} \\
	\frac{\partial Err(w)}{\partial w} &= 2(X^T Xw - X^T Y)
\end{eqn}
\begin{itemize}
	\item Less expensive approach; weights calculated through iterations
	\item Goal is to reduce the weight errors from the previous iteration
	\item Repeat until $\abs{w_{k + 1} - w_k} < \epsilon$
	\item $a_k > 0$ is the learning rate for iteration $k$
	\begin{itemize}
		\item If too large, oscillates forever
		\item If too small, takes longer to reach local minimum
	\end{itemize}
	\item Robbins-Monroe conditions prove convergence:
	\begin{eqn}
		\Sigma_{k = 0:\infty} \alpha_k &= \infty  \\
		\Sigma_{k = 0:\infty} \alpha_k^2 &< \infty
	\end{eqn}
	\item Not that convergence is to local minimum only, not always global
\end{itemize}

\divider

\begin{itemize}
	\item Feature design - if features cannot fully represent a model, we can transform them using non linear functions (eg powers, binary thresholds, etc) into new features. Note that the weights are still linear combinations.
	\item Overfitting - hypothesis explains training data well, but does not generalize to new data
	\item Simple model - high training error, high test error 
	\item Complex model - low training error, high test error 
	\item Training error always goes down with complexity, but at some point in between, test error will be at its lowest
\end{itemize}

\subsection{Validation}

\begin{itemize}
	\item Validation set should be separate from training data 
	\item K-Fold cross validation
	\begin{itemize}
		\item Create $k$ partitions for available data 
		\item For each iteration, train with $k - 1$ subsets, then validate on remaining subset. Repeat $k$ times
		\item Return average prediction error
	\end{itemize}
	\item Leave-One-Out Cross Validation
	\begin{itemize}
		\item K-fold where $k = n$
	\end{itemize}
\end{itemize}

\section{Linear Classification}

\subsection{Binary Classification}

\begin{itemize}
	\item Probabilistic - estimate conditional probability $P(y | x)$ given feature data
	\item Decision boundaries - partition feature spaces into different regions
\end{itemize}

\subsection{Discriminative Models}

\begin{itemize}
	\item Directly estimate $P(y | x)$
	\begin{itemize}
		\item Answers what what the features tell us about the class
		\item Difficult to estimate
	\end{itemize}
	\item Log-odds ratio
	\begin{eqn}
		a = ln \frac{P(y = 1 | x)}{P(y = 0 | x)}
	\end{eqn}
	\begin{itemize}
		\item Outputs likelihood of $y = 1$ vs $y = 0$
		\item Decision boundary is set of points for which $a = 0$
	\end{itemize}
	\item Logistic function - predicted probability for $y = 1$
	\begin{eqn}
		\sigma(w^T x) = \frac{1}{1 + e^{-w^T x}}
	\end{eqn}
	\item Likelihood
	\begin{eqn}
		L(D) &= P(y_1, y_2, ..., y_n | x_1, x_2, ..., x_n, w) \\
		&= \prod_{i = 1}^n \sigma(w^T x_i)^{y_i} (1 - \sigma(w^T x_i))^{1 - y_i}
	\end{eqn}
	\begin{itemize}
		\item Numerically unstable for lots of small numbers
	\end{itemize}
	\item Log-Likelihood
	\begin{eqn}
		l(D) &= ln(L(D)) \\
		&= \Sigma_{i=1}^n y_i ln(\sigma(w^T x_i)) + (1 - y_i)ln(1 - \sigma(w^T x_i))
	\end{eqn}
	\begin{itemize}
		\item Easier to optimize
		\item Negative log-likelihood = cross-entropy loss \\
		Maximizing log-likelihood = minimizing cross-entropy loss
	\end{itemize}
	\item Many kinds of losses exist, eg absolute error, or binary; 
	however, these losses are not always easy to optimize (not differentiable)
	\item Gradient descent update rule
	\begin{eqn}
		w_{k + 1} = w_k + \alpha_k \Sigma_{i = 1:n} x_i(y_i - \sigma(w_k^T x_k))
	\end{eqn}
\end{itemize}

\subsection{Generative Models}

\begin{itemize}
	\item Use Bayes' rule to estimate
	\begin{eqn}
		P(y = 1 | x) = \frac{P(x | y = 1) P(y = 1)}{P(x)}
	\end{eqn}
	\begin{itemize}
		\item Finds the marginal probability of a class
		\item Easy to estimate
	\end{itemize}
	\item Linear Discriminant Analysis (LDA)
	\begin{eqn}
		P(x | y) = \frac{e^{-\frac{1}{2} (x - \mu)^T \Sigma^{-1}(x - \mu)}}{(2 \pi)^{\frac{1}{2}} \abs{\Sigma}^{\frac{1}{2}}}
	\end{eqn}
	\begin{itemize}
		\item Every class assumed to be Gaussian/normally distributed
		\item Both classes have same covariance matrix $\Sigma$, but different means $\mu$
		\item Estimations
		\begin{eqn}
			P(y = 0) &= \frac{N_0}{N_0 + N_1} \\
			P(y = 1) &= \frac{N_1}{N_0 + N_1} \\\\
			\mu_0 &= \frac{\Sigma_{i=1:n} I(y_i = 0) x_i}{N_0} \\
			\mu_1 &= \frac{\Sigma_{i=1:n} I(y_i = 1) x_i}{N_1} \\\\
			\Sigma &= \frac{\Sigma_{k=0:1} \Sigma_{i=1:n} I(y_i = k) (x_i - \mu_i)(x_i - \mu_k)^T}{N_0 + N_1 - 2}
		\end{eqn}
		\begin{itemize}
			\item $N_0$ and $N_1$ are the \# of training samples from classes 1 and 0 respectively
			\item $I(x)$ is an indicator function: $I(x) = 0$ if $x = 0$, $I(x) = 1$ if $x = 1$
		\end{itemize}
		\item Cost is $O(m^2)$ for $m$ classes
	\end{itemize}
	\item Quadratic Discriminant Analysis (QDA)
	\begin{itemize}
		\item Allows different covariance matrices, $\Sigma_y$ for each class $y$
		\item Cost is $O(nm^2)$ for $m$ classes and $n$ features
	\end{itemize}
	\item NaÃ¯ve Bayes 
	\begin{itemize}
		\item Assumes $x_j$ is conditionally independent given $y$ 
		\begin{eqn}
			P(x_j | y) = P(x_j | y, x_k) \forall j, k
		\end{eqn}
		\item Still one $\Sigma_y$ per class, but they are diagonal
		\item Cost is $O(nm)$ for $m$ classes and $n$ features
	\end{itemize}
	\item Laplace smoothing
	\begin{itemize}
		\item Replace maximum likelihood estimator:
		\begin{eqn}
			&\text{Before:} \\
			&Pr(x_j | y = 1) = \frac{(\text{number of instances with } x_j = 1 \text{ and } y = 1)}{(\text{number of examples with } y = 1)} \\
			&\text{After:} \\
			&Pr(x_j | y = 1) = \frac{(\text{number of instances with } x_j = 1 \text{ and } y = 1) + 1}{(\text{number of examples with } y = 1) + 2}
		\end{eqn}
		\item Allows data we previously did not encounter to have a probability greater than 0
	\end{itemize}
\end{itemize}

\section{Evaluation}

\subsection{Evaluating Classification}

\begin{itemize}
	\item True positive (m11) - expect 1, predict 1
	\item False positive (m01) - expect 0, predict 1; type I error
	\item True negative (m00) - expect 0, predict 0
	\item False negative (m10) - expect 1, predict 0; type II error
	\item
	\begin{eqn}
		&\text{Error rate} &&\frac{m01 + m10}{m} \\
		&\text{Accuracy} &&\frac{TP + TN}{ALL} \\
		&\text{Precision/Sensitivity} \qquad &&\frac{TP}{TP + FP} \\
		&\text{Recall} &&\frac{TP}{TP + FN} \\
		&\text{Specificity} &&\frac{TN}{FP + TN} \\
		&\text{False positive rate} &&\frac{FP}{FP + TN} \\ 
		&\text{F1 measure} &&F = 2 \cdot \frac{precision \cdot recall}{precision + recall}
	\end{eqn}

	\item Receiver operating characteristic (ROC)
	\begin{itemize}
		\item Plot true negative and true positive prediction rates based on a moving boundary 
		\item Comparison done by looking at area under ROC curve (AUC)
		\item In a perfect algorithm, $AUC = 1$; for random algorithms, $AUC = 0.5$
	\end{itemize}
\end{itemize}

\subsection{Evaluating Regression}

\begin{itemize}
	\item Mean Square Error 
	\begin{eqn}
		MSE = \frac{1}{n} \Sigma_{i=1}^n (\hat{y}_i - y_i)^2
	\end{eqn}
	\item Root Mean Square Error 
	\begin{eqn}
		RMSE = \sqrt{\frac{1}{n} \Sigma_{i=1}^n (\hat{y}_i - y_i)^2}
	\end{eqn}
	\item Mean Absolute Error
	\begin{eqn}
		MAE = \frac{1}{n} \Sigma_{i=1}^n \abs{\hat{y}_i - y_i}
	\end{eqn}
\end{itemize}

\section{Regularization}

\begin{itemize}
	\item High bias - simple model
	\item High variance - overly complex model
	\item Regularization 
	\begin{itemize}
		\item Reduce overfitting 
		\item Reduce variance at the cost of bias
	\end{itemize}
	\item Ridge Regression (L2-regularization)
	\begin{eqn}
		\hat{w}^{ridge} &= argmin_w (\Sigma_{i=1:n}(y_i - w^T x_i)^2 + \lambda \Sigma_{j=0:m} w_j^2) \\
		&= (X^T X + \lambda I)^{-1} X^T Y
	\end{eqn}
	\begin{itemize}
		\item $\lambda$ can be selected manually or by cross validation
		\item Not equivariant under scaling of data; typically need to normalize inputs first
		\item Can also modify penalty by adding penalty term $2\lambda w$
		\begin{eqn}
			\frac{\partial Err(w)}{\partial w} &= 2(X^T Xw - X^T Y) + \underline{2 \lambda w}
		\end{eqn}
	\end{itemize}
\end{itemize}

\end{document}   