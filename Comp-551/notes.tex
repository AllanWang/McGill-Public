\documentclass[12pt]{article}
	\usepackage{hyperref, fancyhdr, setspace, enumerate}
	\usepackage{tabulary}
	\usepackage{amsmath, amsthm, amssymb, array, keycommand, lastpage, amssymb, xcolor, mathtools}
	\usepackage{multiaudience}
	\usepackage{tabularx}
	\usepackage{makecell}
	\usepackage{enumitem}
	\usepackage[margin=1 in]{geometry}
	\allowdisplaybreaks
	\hypersetup{
		%colorlinks=true, %set true if you want colored links
		linktoc=all, %set to all if you want both sections and subsections linked
		linkcolor=black, %choose some color if you want links to stand out
	}
	\author{Allan Wang} 
	\date{Last updated: \today}
	\title{Comp 551: Machine Learning}
	\pagestyle{fancy}
	\lhead{COMP 551}
	\chead{\leftmark}
	\rhead{Allan Wang}
	\cfoot{Page \thepage \ of \pageref{LastPage}}
	
	% Only number for sections
	\setcounter{secnumdepth}{1}
	
	\newcommand\mm[1]{\begin{pmatrix}#1\end{pmatrix}}

	\setlength{\parindent}{0pt}
	
	\SetNewAudience{notes}
	\SetNewAudience{full}
	
	\setlist[enumerate]{itemsep=0m	m}
	\setlist[itemize]{itemsep=0mm}

	\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
	
	\newcommand{\comment}[1]{}

	\newcommand{\mathcomment}[0]{\quad\color{blue}}

	\newcommand{\bigsum}[2]{\sum\limits_{#1}^{#2}}

	\newcommand{\ddef}[1]{\textcolor{blue}{#1}}
	
	\newcommand{\real}[0]{\mathbb{R}}
	
	\newcommand{\uu}[1]{\underbracket{#1}}

	\newkeycommand{\ccup}[sub=i=1, sup=\infty, base=A_i] {
		\bigcup_{\commandkey{sub}}^{\commandkey{sup}}\commandkey{base}
	}

	\newkeycommand{\ccap}[sub=i=1, sup=\infty, base=A_i] {
		\bigcap_{\commandkey{sub}}^{\commandkey{sup}}\commandkey{base}
	}

	\newkeycommand{\llim}[sub=n \rightarrow \infty, base=A_n] {
		\lim_{\commandkey{sub}}\commandkey{base}
	}

	\newkeycommand{\ssum}[sub=i=1, sup=k] {
		\sum_{\commandkey{sub}}^{\commandkey{sup}}	
	}

	\newenvironment{block}[1][Label]{\underline{#1}\par}{}
%	\newenvironment{proof}{\block[Proof]}{\endblock}
	\newenvironment{proposition}{\block[Proposition]}{\endblock}
	\newenvironment{lemma}{\block[Lemma]}{\endblock}
%	\newenvironment{theorem}{\block[Theorem]}{\endblock}
	\newenvironment{remark}{\block[Remark]}{\endblock}
	\newenvironment{definition}{\block[Definition]}{\endblock}

	\newcommand{\bb}[1]{\left\{#1\right\}}
	\newcommand{\bbb}[1]{\left[#1\right]}
	\newcommand{\pp}[1]{\left(#1\right)}
	\newcommand{\abs}[1]{\left|#1\right|}

	\newcommand{\divider}[0]{\par\textcolor{lightgray}{\rule{\textwidth}{0.1pt}}}
	
	\newenvironment{claim}{\textit{Claim:}}{\hfill $\square$}
	
	\newenvironment{remarks}{\underline{Remarks}\par}{}
	
	\newenvironment{example}{\shownto{-,notes}\underline{Example}\par}{\par\divider\endshownto}
	
	\newenvironment{eqn}{\equation\alignedat{3}}{\endalignedat\endequation}
	
	\newcommand{\todo}[0]{\textcolor{red}{\textbackslash\textbackslash TODO \ }}
	
	\newcounter{theorem}
	\newcommand{\theorem}[1]{\refstepcounter{theorem}\par\medskip
		\underline{Theorem~\thetheorem. #1}}
	
	\let\oldperp\perp
	\renewcommand{\perp}[0]{\oldperp\!\!\!\oldperp}

\begin{document}
\onehalfspacing
\maketitle
\tableofcontents
\pagebreak

\section{Linear Regression}

\begin{itemize}
	\item Classification - discrete set output
	\item Regression - continuous output
	\item Supervised learning - given training examples with labels, find model to predict labels given input
	\item i.i.d assumption - training set is assumed to be \textit{independently} and \textit{identically distributed}
	\item Linear hypothesis - find weights to minimize
	\begin{eqn}
		Err() = \Sigma_{i = 1:n} (y_i - w^T x_i)^2
	\end{eqn}
\end{itemize}

\subsection{Least-Squares Error}

\begin{eqn}
	f_w &= argmin \Sigma_{i=1:n}(y_i - w^T x_i)^2 \\
	\hat{w} &= (X^T X)^{-1} X^T Y
\end{eqn}
\begin{itemize}
	\item Note that both w and x vectors have an extra dimension (m + 1) for a dummy/intercept term (all 1s for x)
	\item Computational cost is $O(m^3 + nm^2)$ for n inputs and m features
	\item Only works if $X^T X$ is nonsingular (no correlation between features)
	\begin{itemize}
		\item If a feature is a linear function of others, you can drop it
		\item If the number of features exceeds number of training examples, you can reduce the number of features
	\end{itemize}
\end{itemize}

\subsection{Gradient Descent}

\begin{eqn}
	w_{k + 1} = w_k - \alpha_k \frac{\partial Err(w_k)}{\partial w_k}
\end{eqn}
\begin{itemize}
	\item Less expensive approach; weights calculated through iterations
	\item Goal is to reduce the weight errors from the previous iteration
	\item Repeat until $\abs{w_{k + 1} - w_k} < \epsilon$
	\item $a_k > 0$ is the learning rate for iteration $k$
	\begin{itemize}
		\item If too large, oscillates forever
		\item If too small, takes longer to reach local minimum
	\end{itemize}
	\item Robbins-Monroe conditions prove convergence:
	\begin{eqn}
		\Sigma_{k = 0:\infty} \alpha_k &= \infty  \\
		\Sigma_{k = 0:\infty} \alpha_k^2 &< \infty
	\end{eqn}
	\item Not that convergence is to local minimum only, not always global
\end{itemize}

\divider

\begin{itemize}
	\item Feature design - if features cannot fully represent a model, we can transform them using non linear functions (eg powers, binary thresholds, etc) into new features. Note that the weights are still linear combinations.
	\item Overfitting - hypothesis explains training data well, but does not generalize to new data
	\item Simple model - high training error, high test error 
	\item Complex model - low training error, high test error 
	\item Training error always goes down with complexity, but at some point in between, test error will be at its lowest
\end{itemize}

\subsection{Validation}

\begin{itemize}
	\item Validation set should be separate from training data 
	\item K-Fold cross validation
	\begin{itemize}
		\item Create $k$ partitions for available data 
		\item For each iteration, train with $k - 1$ subsets, then validate on remaining subset. Repeat $k$ times
		\item Return average prediction error
	\end{itemize}
	\item Leave-One-Out Cross Validation
	\begin{itemize}
		\item K-fold where $k = n$
	\end{itemize}
\end{itemize}

\section{Linear Classification}

\subsection{Binary Classification}

\begin{itemize}
	\item High level
	\begin{itemize}
		\item Probabilistic - estimate conditional probability $P(y | x)$ given feature data
		\item Decision boundaries - partition feature spaces into different regions
	\end{itemize}
	\item Approaches
	\begin{itemize}
		\item Discriminative - directly estimate $P(y | x)$
		\item Generative - use Bayes' rule to estimate
		\begin{eqn}
			P(y = 1 | x) = \frac{P(x | y = 1) P(y = 1)}{P(x)}
		\end{eqn}
	\end{itemize}
	\item Log-odds ratio
	\begin{eqn}
		a = ln \frac{P(y = 1 | x)}{P(y = 0 | x)}
	\end{eqn}
	\begin{itemize}
		\item Outputs likelihood of $y = 1$ vs $y = 0$
		\item Decision boundary is set of points for which $a = 0$
	\end{itemize}
	\item Logistic function - predicted probability for $y = 1$
	\begin{eqn}
		\sigma = \frac{1}{1 + e^{-a}}
	\end{eqn}
	\item Likelihood
	\begin{eqn}
		L(D) &= P(y_1, y_2, ..., y_n | x_1, x_2, ..., x_n, w) \\
		&= \prod_{i = 1}^n \sigma(w^T x_i)^{y_i} (1 - \sigma(w^T x_i))^{1 - y_i}
	\end{eqn}
\end{itemize}

\newpage

\divider

\section{Formulas}

\begin{itemize}
	\item Least squares 
	\begin{eqn}
		f_w = argmin \Sigma_{i=1:n}(y_i - w^T x_i)^2 \\
		\hat{w} = (X^T X)^{-1} X^T Y
	\end{eqn}
	\item Bayes' rule 
	\begin{eqn}
		P(y = 1 \mid x) = \frac{P(x \mid y = 1)P(y = 1)}{P(x)}
	\end{eqn}
\end{itemize}

\subsection{LDA}

\begin{eqn}
	P(y = 0) = \frac{N_0}{N_0 + N_1} \\
	\mu_0 = \sum_{i=1:n}{I(y_i = 0) \frac{x_i}{N_0}} \\
	\Sigma = \sum_{k = 0:1} \sum_{i = 1:n} \frac{I(y_i = 0)(x_i - \mu_k)(x_i - \mu_k)^T}{N_0 + N_1 - N_k}
\end{eqn} 

LDA assumes that 

\begin{eqn}
	P(x \mid y) = \frac{e^{-\frac{1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu)}}{(2 \pi)^{\frac{1}{2}} \abs{\Sigma}^{\frac{1}{2}}}
\end{eqn}

Two class case 

\begin{eqn}
	P(y = 0) &= \frac{N_0}{N_0 + N_1} \\ 
	P(y = 1) &= \frac{N_1}{N_0 + N_1} \\
	\mu_0 &= \Sigma_{i = 1:n} I(y_i = 0) \frac{x_i}{N_0} \\
	\mu_1 &= \Sigma_{i = 1:n} I(y_i = 1) \frac{x_i}{N_1} \\\\
	x = 0 &\rightarrow I(x) = 0 \\
	x = 1 &\rightarrow I(x) = 1
\end{eqn}

\section{Linear Classification}

\end{document}   