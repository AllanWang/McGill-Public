\documentclass[12pt]{article}
	\usepackage{hyperref, fancyhdr, setspace, enumerate}
	\usepackage{tabulary}
	\usepackage{enumitem}
	\usepackage{amsmath, amsthm, amssymb, array, keycommand, lastpage, amssymb, xcolor, mathtools}
	\usepackage{multiaudience}
	\usepackage{tabularx}
	\usepackage{makecell}
	\usepackage[margin=1 in]{geometry}
	\allowdisplaybreaks
	\hypersetup{
		%colorlinks=true, %set true if you want colored links
		linktoc=all, %set to all if you want both sections and subsections linked
		linkcolor=black, %choose some color if you want links to stand out
	}
	\author{Allan Wang} 
	\date{Last updated: \today}
	\title{Comp 551: Machine Learning}
	\pagestyle{fancy}
	\lhead{COMP 551}
	\chead{\leftmark}
	\rhead{Allan Wang}
	\cfoot{Page \thepage \ of \pageref{LastPage}}
	
	% Only number for sections
	\setcounter{secnumdepth}{1}
	
	\newcommand\mm[1]{\begin{pmatrix}#1\end{pmatrix}}

	\setlength{\parindent}{0pt}
	
	\SetNewAudience{notes}
	\SetNewAudience{full}
	
	\setlist[enumerate]{itemsep=0mm}
	\setlist[itemize]{itemsep=0mm}

	% \newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
	
	\newcommand{\comment}[1]{}

	\newcommand{\mathcomment}[0]{\quad\color{blue}}

	\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
	
	\newcommand{\real}[0]{\mathbb{R}}
	
	\newcommand{\uu}[1]{\underbracket{#1}}

	\newcommand{\bb}[1]{\left\{#1\right\}}
	\newcommand{\bbb}[1]{\left[#1\right]}
	\newcommand{\pp}[1]{\left(#1\right)}
	\newcommand{\abs}[1]{\left|#1\right|}

	\newcommand{\divider}[0]{\par\textcolor{lightgray}{\rule{\textwidth}{0.1pt}}}
	
	\newenvironment{eqn}{\equation\alignedat{3}}{\endalignedat\endequation}
	
	\newcommand{\todo}[0]{\textcolor{red}{\textbackslash\textbackslash TODO \ }}
	
\begin{document}
\onehalfspacing
\maketitle
\tableofcontents
\pagebreak

\section{Linear Regression}

\begin{itemize}
	\item Classification - discrete set output
	\item Regression - continuous output
	\item Supervised learning - given training examples with labels, find model to predict labels given input
	\item i.i.d assumption - training set is assumed to be \textit{independently} and \textit{identically distributed}
	\item Linear hypothesis - find weights to minimize
	\begin{eqn}
		Err() = \Sigma_{i = 1:n} (y_i - w^T x_i)^2
	\end{eqn}
\end{itemize}

\subsection{Least-Squares Error}

\begin{eqn}
	f_w &= argmin \Sigma_{i=1:n}(y_i - w^T x_i)^2 \\
	\hat{w} &= (X^T X)^{-1} X^T Y
\end{eqn}
\begin{itemize}
	\item Note that both w and x vectors have an extra dimension (m + 1) for a dummy/intercept term (all 1s for x)
	\item Computational cost is $O(m^3 + nm^2)$ for n inputs and m features
	\item Only works if $X^T X$ is nonsingular (no correlation between features)
	\begin{itemize}
		\item If a feature is a linear function of others, you can drop it
		\item If the number of features exceeds number of training examples, you can reduce the number of features
	\end{itemize}
\end{itemize}

\subsection{Gradient Descent}

\begin{eqn}
	w_{k + 1} &= w_k - \alpha_k \frac{\partial Err(w_k)}{\partial w_k} \\
	\frac{\partial Err(w)}{\partial w} &= 2(X^T Xw - X^T Y)
\end{eqn}
\begin{itemize}
	\item Less expensive approach; weights calculated through iterations
	\item Goal is to reduce the weight errors from the previous iteration
	\item Repeat until $\abs{w_{k + 1} - w_k} < \epsilon$
	\item $a_k > 0$ is the learning rate for iteration $k$
	\begin{itemize}
		\item If too large, oscillates forever
		\item If too small, takes longer to reach local minimum
	\end{itemize}
	\item Robbins-Monroe conditions prove convergence:
	\begin{eqn}
		\Sigma_{k = 0:\infty} \alpha_k &= \infty  \\
		\Sigma_{k = 0:\infty} \alpha_k^2 &< \infty
	\end{eqn}
	\item Not that convergence is to local minimum only, not always global
\end{itemize}

\divider

\begin{itemize}
	\item Feature design - if features cannot fully represent a model, we can transform them using non linear functions (eg powers, binary thresholds, etc) into new features. Note that the weights are still linear combinations.
	\item Overfitting - hypothesis explains training data well, but does not generalize to new data
	\item Simple model - high training error, high test error 
	\item Complex model - low training error, high test error 
	\item Training error always goes down with complexity, but at some point in between, test error will be at its lowest
\end{itemize}

\subsection{Validation}

\begin{itemize}
	\item Validation set should be separate from training data 
	\item K-Fold cross validation
	\begin{itemize}
		\item Create $k$ partitions for available data 
		\item For each iteration, train with $k - 1$ subsets, then validate on remaining subset. Repeat $k$ times
		\item Return average prediction error
	\end{itemize}
	\item Leave-One-Out Cross Validation
	\begin{itemize}
		\item K-fold where $k = n$
	\end{itemize}
\end{itemize}

\section{Linear Classification}

\subsection{Binary Classification}

\begin{itemize}
	\item Probabilistic - estimate conditional probability $P(y | x)$ given feature data
	\item Decision boundaries - partition feature spaces into different regions
\end{itemize}

\subsection{Discriminative Models}

\begin{itemize}
	\item Directly estimate $P(y | x)$
	\begin{itemize}
		\item Answers what what the features tell us about the class
		\item Difficult to estimate
	\end{itemize}
	\item Log-odds ratio
	\begin{eqn}
		a = ln \frac{P(y = 1 | x)}{P(y = 0 | x)}
	\end{eqn}
	\begin{itemize}
		\item Outputs likelihood of $y = 1$ vs $y = 0$
		\item Decision boundary is set of points for which $a = 0$
	\end{itemize}
	\item Logistic function - predicted probability for $y = 1$
	\begin{eqn}
		\sigma(w^T x) = \frac{1}{1 + e^{-w^T x}}
	\end{eqn}
	\item Likelihood
	\begin{eqn}
		L(D) &= P(y_1, y_2, ..., y_n | x_1, x_2, ..., x_n, w) \\
		&= \prod_{i = 1}^n \sigma(w^T x_i)^{y_i} (1 - \sigma(w^T x_i))^{1 - y_i}
	\end{eqn}
	\begin{itemize}
		\item Numerically unstable for lots of small numbers
	\end{itemize}
	\item Log-Likelihood
	\begin{eqn}
		l(D) &= ln(L(D)) \\
		&= \Sigma_{i=1}^n y_i ln(\sigma(w^T x_i)) + (1 - y_i)ln(1 - \sigma(w^T x_i))
	\end{eqn}
	\begin{itemize}
		\item Easier to optimize
		\item Negative log-likelihood = cross-entropy loss \\
		Maximizing log-likelihood = minimizing cross-entropy loss
	\end{itemize}
	\item Many kinds of losses exist, eg absolute error, or binary; 
	however, these losses are not always easy to optimize (not differentiable)
	\item Gradient descent update rule
	\begin{eqn}
		w_{k + 1} = w_k + \alpha_k \Sigma_{i = 1:n} x_i(y_i - \sigma(w_k^T x_k))
	\end{eqn}
\end{itemize}

\subsection{Generative Models}

\begin{itemize}
	\item Use Bayes' rule to estimate
	\begin{eqn}
		P(y = 1 | x) = \frac{P(x | y = 1) P(y = 1)}{P(x)}
	\end{eqn}
	\begin{itemize}
		\item Finds the marginal probability of a class
		\item Easy to estimate
	\end{itemize}
	\item Linear Discriminant Analysis (LDA)
	\begin{eqn}
		P(x | y) = \frac{e^{-\frac{1}{2} (x - \mu)^T \Sigma^{-1}(x - \mu)}}{(2 \pi)^{\frac{1}{2}} \abs{\Sigma}^{\frac{1}{2}}}
	\end{eqn}
	\begin{itemize}
		\item Every class assumed to be Gaussian/normally distributed
		\item Both classes have same covariance matrix $\Sigma$, but different means $\mu$
		\item Estimations
		\begin{eqn}
			P(y = 0) &= \frac{N_0}{N_0 + N_1} \\
			P(y = 1) &= \frac{N_1}{N_0 + N_1} \\\\
			\mu_0 &= \frac{\Sigma_{i=1:n} I(y_i = 0) x_i}{N_0} \\
			\mu_1 &= \frac{\Sigma_{i=1:n} I(y_i = 1) x_i}{N_1} \\\\
			\Sigma &= \frac{\Sigma_{k=0:1} \Sigma_{i=1:n} I(y_i = k) (x_i - \mu_i)(x_i - \mu_k)^T}{N_0 + N_1 - 2}
		\end{eqn}
		\begin{itemize}
			\item $N_0$ and $N_1$ are the \# of training samples from classes 1 and 0 respectively
			\item $I(x)$ is an indicator function: $I(x) = 0$ if $x = 0$, $I(x) = 1$ if $x = 1$
		\end{itemize}
		\item Cost is $O(m^2)$ for $m$ classes
	\end{itemize}
	\item Quadratic Discriminant Analysis (QDA)
	\begin{itemize}
		\item Allows different covariance matrices, $\Sigma_y$ for each class $y$
		\item Cost is $O(nm^2)$ for $m$ classes and $n$ features
	\end{itemize}
	\item NaÃ¯ve Bayes 
	\begin{itemize}
		\item Assumes $x_j$ is conditionally independent given $y$ 
		\begin{eqn}
			P(x_j | y) = P(x_j | y, x_k) \forall j, k
		\end{eqn}
		\item Still one $\Sigma_y$ per class, but they are diagonal
		\item Cost is $O(nm)$ for $m$ classes and $n$ features
	\end{itemize}
	\item Laplace smoothing
	\begin{itemize}
		\item Replace maximum likelihood estimator:
		\begin{eqn}
			&\text{Before:} \\
			&Pr(x_j | y = 1) = \frac{(\text{number of instances with } x_j = 1 \text{ and } y = 1)}{(\text{number of examples with } y = 1)} \\
			&\text{After:} \\
			&Pr(x_j | y = 1) = \frac{(\text{number of instances with } x_j = 1 \text{ and } y = 1) + 1}{(\text{number of examples with } y = 1) + 2}
		\end{eqn}
		\item Allows data we previously did not encounter to have a probability greater than 0
	\end{itemize}
\end{itemize}

\section{Evaluation}

\subsection{Evaluating Classification}

\begin{itemize}
	\item True positive (m11) - expect 1, predict 1
	\item False positive (m01) - expect 0, predict 1; type I error
	\item True negative (m00) - expect 0, predict 0
	\item False negative (m10) - expect 1, predict 0; type II error
	\item
	\begin{eqn}
		&\text{Error rate} &&\frac{m01 + m10}{m} \\
		&\text{Accuracy} &&\frac{TP + TN}{ALL} \\
		&\text{Precision} \qquad &&\frac{TP}{TP + FP} \\
		&\text{Recall/Sensitivity} &&\frac{TP}{TP + FN} \\
		&\text{Specificity} &&\frac{TN}{FP + TN} \\
		&\text{False positive rate} &&\frac{FP}{FP + TN} \\ 
		&\text{F1 measure} &&F = 2 \cdot \frac{precision \cdot recall}{precision + recall}
	\end{eqn}

	\item Receiver operating characteristic (ROC)
	\begin{itemize}
		\item Plot true negative and true positive prediction rates based on a moving boundary 
		\item Comparison done by looking at area under ROC curve (AUC)
		\item In a perfect algorithm, $AUC = 1$; for random algorithms, $AUC = 0.5$
	\end{itemize}
\end{itemize}

\subsection{Evaluating Regression}

\begin{itemize}
	\item Mean Square Error 
	\begin{eqn}
		MSE = \frac{1}{n} \Sigma_{i=1}^n (\hat{y}_i - y_i)^2
	\end{eqn}
	\item Root Mean Square Error 
	\begin{eqn}
		RMSE = \sqrt{\frac{1}{n} \Sigma_{i=1}^n (\hat{y}_i - y_i)^2}
	\end{eqn}
	\item Mean Absolute Error
	\begin{eqn}
		MAE = \frac{1}{n} \Sigma_{i=1}^n \abs{\hat{y}_i - y_i}
	\end{eqn}
\end{itemize}

\section{Regularization}

\begin{itemize}
	\item High bias - simple model
	\item High variance - overly complex model
	\item Regularization 
	\begin{itemize}
		\item Reduce overfitting 
		\item Reduce variance at the cost of bias
	\end{itemize}
\end{itemize}

\subsection{L2 Regularization}

\begin{itemize}
	\item Aka ridge regression
	\begin{eqn}
		\hat{w}^{ridge} &= argmin_w (\Sigma_{i=1:n}(y_i - w^T x_i)^2 + \lambda \Sigma_{j=0:m} w_j^2) \\
		&= (X^T X + \lambda I)^{-1} X^T Y
	\end{eqn}
	\begin{itemize}
		\item $\lambda$ can be selected manually or by cross validation
		\item Not equivariant under scaling of data; typically need to normalize inputs first
		\item Can also modify penalty by adding penalty term $2\lambda w$
		\begin{eqn}
			\frac{\partial Err(w)}{\partial w} &= 2(X^T Xw - X^T Y) + \underline{2 \lambda w}
		\end{eqn}
		\item Tends to lower all weights
	\end{itemize}
\end{itemize}

\subsection{L1 Regularization}

\begin{itemize}
	\item Aka lasso regression
	\begin{eqn}
		\hat{w}^{lasso} = argmin_w (\Sigma_{i=1:n}(y_i - w^T x_i)^2 + \lambda \Sigma_{j=1:m} \abs{w_j})
	\end{eqn}
	\item More computationally expensive than L2
	\item Sets the weights of less relevant input features to 0
	\item Can also modify penalty by adding penalty term $\lambda sign(w)$ where 
	\begin{itemize}
		\item $sign(x) = 1$ if $x > 0$
		\item $sign(x) = 0$ if $x = 0$
		\item $sign(x) = -1$ if $x < 0$
	\end{itemize}
\end{itemize}

\divider

\begin{itemize}
	\item Elastic net combines L2 and L1
\end{itemize}

\section{Decision Trees}

\begin{itemize}
	\item Nodes represent partitions
	\item Internal nodes are tests based on different features
	\begin{itemize}
		\item Typically branch on all possibilities for discrete features
		\item Typically branch on threshold values for continuous features
	\end{itemize}
	\item Leaf nodes include set of training examples satisfying all tests along the branch
	\item Can express every boolean function; goal is to encode functions compactly
	\item Typically constructed sing recursive top-down procedure + pruning to avoid overfitting
	\item Advantages: fast, easy to interpret, accurate 
	\item Disadvantages: sensitive to outliers, bad for learning functions with smooth, curvilinear boundaries
	\item Approach
	\begin{enumerate}
		\item If training instances have same class, create leaf with that class label and exit 
		\item Else, pick best test to split on 
		\item Split training data 
		\item Recurse on 1-3 for each subset
	\end{enumerate}
\end{itemize}

\subsection{Finding Best Test}

\begin{itemize}
	\item If event $E$ with probability $P(E)$ has occurred with certainty, we received $I(E)$ bits of information, where
	\begin{eqn}
		I(E) = log_2 \frac{1}{P(E)}
	\end{eqn}
	\item Events with smaller probabilities produce more bits of information 
	\item Entropy (H(S))- average amount of information from source $S$, which emits symbols $\{s_1, ..., s_k\}$ with probabilities $\{p_1, ..., p_k\}$
	\begin{eqn}
		H(S) = \Sigma_i p_i I(s_i) = - \Sigma_i p_i log_2 p_i
	\end{eqn}
	\item Conditional entropy
	\begin{eqn}
		H(y | x) = \Sigma_v P(x = v) H(y | x = v)
	\end{eqn}
	\item Information gain - reduction in entropy obtained by knowing $x$
	\begin{eqn}
		IG(x) = H(D) - H(D | x)
	\end{eqn}
	\item Binary Classification 
	\begin{itemize}
		\item Given $p$ positive samples and $n$ negative samples:
		\begin{eqn}
			H(D) = -\frac{p}{p + n} log_2 \frac{p}{p + n} - \frac{n}{p + n} log_2 \frac{n}{p + n}
		\end{eqn}
		\item Example: Given 40 examples with 30 positive and 10 negative, as well as a test $T$ that divides to [15+, 7-] for true and [15+, 3-] for false, we obtain entropy 
		\begin{eqn}
			H(D | T) = \frac{22}{40}\left[ -\frac{15}{22} log_2 \frac{15}{22} - \frac{7}{22} log_2 \frac{7}{22} \right] + \frac{18}{40} \left[ -\frac{15}{18} log_2 \frac{15}{18} - \frac{3}{18} log_2 \frac{3}{18} \right] = 0.788
		\end{eqn}
	\end{itemize}
	\item For classification, choose test with highest information gain 
	\item For regression, choose test with lowest MSE
	\item For real-valued features, can select possible thresholds based on midpoints of data values
\end{itemize}

\subsection{Avoiding Overfitting}

\begin{itemize}
	\item Remove some nodes for better generalization 
	\item Early stopping - stop growing tree if splits do not improve information gain of validation step 
	\item Post pruning - grow full tree then remove lower nodes with low information gain on validation set (generally better)
	\begin{itemize}
		\item For each node:
		\item Evaluate validation set accuracy if subtree is pruned 
		\item Greedily remove nodes that most improve validation set accuracy 
		\item Replace removed node by leaf with majority class of corresponding examples 
		\item Stop when pruning hurts validation set accuracy
	\end{itemize}
\end{itemize}

\section{Features}

\begin{itemize}
	\item Strategies
	\begin{itemize}
		\item Domain knowledge to construct ``add hoc'' features 
		\item Normalization 
		\item Non-linear expansions 
		\item Regularization
	\end{itemize}
	\item NLP Features
	\begin{itemize}
		\item Words (binary, absolute frequency, relative frequency)
		\item TF-IDF 
		\item N-grams
		\item Syntactic features
		\item Word embeddings
		\item Stopwords (common words that may not provide much information)
		\item Lemmatization (inflectional morphology)
	\end{itemize}
	\item Approaches
	\begin{itemize}
		\item Wrapper \& filter - applied during preprocessing
		\item Embedded - integrated in optimization method (eg regularization)
	\end{itemize}
\end{itemize}

\subsection{Principal Component Analysis (PCA)}

\begin{itemize}
	\item Project data into lower-dimensional sub-spaces
	\item Solves
	\begin{eqn}
		argmin_{W, U} \Sigma_{i=1:n} \norm{X - XWU^T}^2
	\end{eqn}
	\item Solution $W$ given by eigen-decomposition of $X^T X$; columns are orthogonal
\end{itemize}

\subsection{Variable Ranking}

\begin{itemize}
	\item Rank features using scoring features; select the highest ranked features
	\item Simple \& fast, but requires a scoring function
	\item Scoring functions 
	\begin{itemize}
		\item Correlation
		\item Mutual information (finds  nonlinear relationships)
	\end{itemize}
\end{itemize}

\section{Instance Learning}

\begin{itemize}
	\item Parametric supervised learning 
	\begin{itemize}
		\item Assumes we have labelled examples
		\item Learns parameter vector of fixed size
	\end{itemize}
	\item Non-parametric learning
	\begin{itemize}
		\item Store all training examples $\langle x_i, y_i \rangle$
		\item With new query, compute value based on most similar points
		\item Requires distance function (eg euclidian distance)
		\begin{itemize}
			\item Often domain specific 
			\item May require feature preprocessing 
			\item Can sometimes be learned
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{One-Nearest Neighbour}

\begin{itemize}
	\item Lazy learning - generalization upon query
	\item Requires no learning (just store data)
	\item For new point $x_{new}$, find nearest sample $x_{i^*} = argmin_i d(x_i, x)$ and predict $y_{new} = y_{i^*}$
	\item Essentially results in a decision boundary where each boundary is equidistance between two points of opposite classes
\end{itemize}

\subsection{K-Nearest Neighbour (KNN)}

\begin{itemize}
	\item Requires no learning (just store data)
	\item For new point $x_{new}$, find $k$ nearest training samples 
	\item For regression, predict mean/median of neighbouring $y$ values 
	\item For classification, predict majority or empirical probability of each class
	\item Low k results in low bias and high variance 
	\item High k results in high bias and low variance 
	\item Sensitive to small variations
\end{itemize}

\subsection{Distance-Weighted (kernel-based) NN}

\begin{itemize}
	\item For new point $x_{new}$, compute $w_i = w(d(x_i, x_{new}))$ for all $x_i$ in training data, where $d$ is a weighting function and predict $y = \dfrac{\Sigma_i w_i y_i}{\Sigma_i w_i}$
	\item Example of a weighting function is the inverse euclidean distance
\end{itemize}

\divider

\begin{itemize}
	\item Instance based learning is useful when
	\begin{itemize}
		\item A good distance metric exists 
		\item There aren't too many attributes per instance (otherwise distances will be similar, leading to noise)
	\end{itemize}
\end{itemize}

\section{Support Vector Machines (SVM)}

\subsection{Perceptron}

\begin{eqn}
	h_w(x) = sign(w^T x)
\end{eqn}

\begin{itemize}
	\item Outputs +1 if $w^T x \ge 0$, -1 otherwise
	\item Decision boundary is $w^T x = 0$
	\item Gradient-descent learning
	\begin{eqn}
		Err(w) = \Sigma_{i=1:n} \left\{ 0 \text{ if } y_i w^T x_i \ge 0; -y_i w^T x \text{ otherwise} \right\}
	\end{eqn}
	\begin{itemize}
		\item Error is 0 for correctly classified example
		\item Error shows how much to shift for incorrectly classified example
		\item Error is 0 if all examples are classified correctly
	\end{itemize}
	\item Learning converges within finite number of updates for linearly separable datasets; otherwise, there will be oscillation
	\item Dual representation
	\begin{eqn}
		w = \Sigma_{i=1:n} \alpha_i y_i x_i
	\end{eqn}
	\begin{itemize}
		\item $\alpha_i$ is sum of step sizes used for all updates applied to example $i$
	\end{itemize}
	\item Solution often non-unique; depends on order of updates
\end{itemize}

\subsection{Linear SVM}

\begin{itemize}
	\item Choose $w$ such that margin is maximized
	\item Margin is twice the Euclidean distance from hyper-plane to nearest training example
	\item Let $\gamma_i$ be the distance from $x_i$ to decision boundary, connected at point $x_i^0$. Let $w$ be the normal to the decision boundary. We can define:
	\begin{eqn}
		x_i^0 = x_i - \frac{\gamma_i w}{\norm{w}}
	\end{eqn}
	\begin{itemize}
		\item $\dfrac{w}{\norm{w}}$ is the unit normal 
		\item $\gamma_i$ is the scalar distance from $x_i$ to $x_i^0$
	\end{itemize}
	\item Goal is to minimize $\frac{1}{2} \norm{w}^2$ with respect to $w$ such that $y_i w^T x_i \ge 1$ 
	\item \todo{Lagrangian optimization; 439}
	\item Support vectors - points lying on edge of margin, affecting the decision boundary
	\item \todo{Hinge loss; 461}
	\item Multiple classes 
	\begin{itemize}
		\item One-vs-all - learn K separate binary classifiers
		\begin{itemize}
			\item Can be inconsistent
			\item Training sets are imbalanced
		\end{itemize}
		\item Multi-class SVM
	\end{itemize}
\end{itemize}

\section{Ensembles}

Run base algorithms multiple times, then combine predictions for a final prediction

\subsection{Bagging}

\begin{itemize}
	\item Create K independent models by training same base algorithm on different subsets of training data
	\item Reduces variance, increases bias
	\item Good for ``reasonable'' classifiers
	\item Bootstrapping - cross-validation but using random sampling with replacement
	\begin{itemize}
		\item For new dataset, randomly select value from full dataset with replacement until size is sufficient
	\end{itemize}
	\item Incremental construction
	\begin{itemize}
		\item Hypotheses should not be constructed independently
		\item New hypotheses should focus on problematic instances
	\end{itemize}
	\item Random forest
	\begin{itemize}
		\item Use K bootstrap replicates to train K different trees
		\item For each node, pick $m$ variables at random (where $m < M$, $M$ being the total number of features)
		\item Determine best test using normalized information gain
		\item Recurse until depth is reached without pruning
		\item Resulting trees have high variance, but ensemble uses average, which reduces variance
		\item Fast \& can handle lots of data 
		\item Can circumvent difficulties in optimization
		\item Low interpretability
		\item New predictions may be more expensive (need to go through all trees)
	\end{itemize}
\end{itemize}

\subsection{Boosting}

\begin{itemize}
	\item Incrementally train K models, where each successive model tries to fix mistakes of previous models
	\item Reduces bias \& increases classification margin
	\item Tends not to overfit 
	\item Good for very simple classifiers
	\item Problematic if lots of data is mislabelled
	\item Algorithm 
	\begin{itemize}
		\item Train simple predictor
		\item Re-weight training examples, putting more weight on those that were not properly classified 
		\item Repeat n times
	\end{itemize}
	\item Adaboost adapts error rate to reduce training error exponentially fast
\end{itemize}

\subsection{Stacking}

\begin{itemize}
	\item Stacking - train K different models and combine their output
	\item Predictions from base models become features of new meta-model
	\item Works well when base models have complimentary strengths and weaknesses
\end{itemize}

\section{Neural Networks}

\begin{itemize}
	\item A single perceptron can represent linear boundaries, but cannot represent non-linearly separated functions like XOR. 
	However, stacking perceptrons to create new datasets allows more functions to become linearly separable
	\item Sigmoid function provides ``soft threshold'', whereas perceptron provides ``hard threshold''
	\item Feed-forward neural network
	\begin{itemize}
		\item Connections from one layer to the next in one direction; no connections between units of the same layer
		\item Hidden units are $h_i = \sigma(w_i^T x + b_i), \forall i$
		\item Unlike stacking, we jointly train the entire network
		\item Can have multiple output units
		\item Depth - number of layers
		\item Fully-connected - if all units in layer $j$ provide input to all units in layer $j + 1$
	\end{itemize}
	\item Neural network with two hidden layers can approximate any function to arbitrary accuracy
	\item Activation function can be an arbitrary non-linear function, not just limited to sigmoid
\end{itemize}

\subsection{Gradient Descent}

\begin{itemize}
	\item Used to minimize errors, as hypothesis is differentiable and too complex to compute optimal weights directly 
	\item Stochastic gradient descent
	\begin{enumerate}
		\item Initialize weights to small random numbers 
		\item Pick single training example $x$
		\item Feed example then compute correction
		\item Compute correction for output unit
		\begin{eqn}
			\frac{\partial J}{\partial w_{out}} = \delta_{out} x
		\end{eqn}
		\item Compute correction for each hidden unit $j$ 
		\begin{eqn}
			\frac{\partial J}{\partial w_j} = \delta_{out} w_{out, j} \sigma(w_j^T x + b)(1 - \sigma(w_j^T x + b))x
		\end{eqn}
		\item Update each network weight
		\begin{eqn}
			w_j &= w_j - \alpha \frac{\partial J}{\partial w_j} \forall j \\ 
			w_{out} &= w_{out} - \alpha \frac{\partial J}{\partial w_{out}}
		\end{eqn}
		\item Repeat from step 2 until convergence
	\end{enumerate}
	\item Batch gradient descent
	\begin{itemize}
		\item Compute error on all examples 
		\item Loop through training data, accumulating weight changes 
		\item Update all weights and repeat
	\end{itemize}
	\item Mini-batch gradient descent
	\begin{itemize}
		\item Compute error on randomly selected subset
	\end{itemize}
\end{itemize}

\section{Backpropagation}

\subsection{Activation Functions}

\begin{itemize}
	\item Sigmoid
	\begin{eqn}
		\phi(z) = \frac{1}{1 + e^{-z}}
	\end{eqn}
	\begin{itemize}
		\item Easy to differentiate 
		\item Easily saturates; inputs outside of [-4, 4] remain essentially constant, making it hard to train
	\end{itemize}
	\item Tanh 
	\begin{eqn}
		\phi(z) = tanh z = \frac{e^z - e^{-z}}{e^z + e^{-z}}
	\end{eqn}
	\begin{itemize}
		\item Easy to differentiate ($1 - tanh^2(z)$)
		\item Slightly less prone to saturation than sigmoid
	\end{itemize}
	\item ReLU 
	\begin{eqn}
		ReLU(z) = max(z, 0)
	\end{eqn}
	\begin{itemize}
		\item Unbounded range; never saturates 
	\item De facto standard
	\end{itemize}
	\item Softplus
	\begin{eqn}
		softplus(z) = ln(1 + e^x)
	\end{eqn}
	\begin{itemize}
		\item Similar to ReLU but smoother
		\item Expensive derivative compared to ReLU
	\end{itemize}
\end{itemize} 

\subsection{Derivatives}

\begin{itemize}
	\item Tell us how much we impact another node if we change one node by one unit
	\item For non-neighbouring nodes, sum over all possible paths from one node to the other, multiplying derivatives on each edge of the path
	\item Computationally intensive, so we can instead factor paths by summing derivatives of all inputs to a node first
	\item Forward mode - start from source, and at each node, sum all incoming edges/derivatives
	\item Reverse mode - start from sink, and at each node, sum all outgoing edges/derivatives
	\item Both only touch each edge once, but since we want derivatives of the output/loss with respect to all previous layers, reverse mode is better 
	\item Reverse-mode automatic differentiation (RV-AD) = backpropagation
\end{itemize}

\divider

\begin{itemize}
	\item Guaranteed convergence to local minimum 
	\item Use random restarts to find better minimum 
	\item Overtraining occurs if weights take on large magnitudes; \# of training updates is also a hyper-parameter 
	\item Adaptive optimization algorithms change learning rates for each parameter based on history of gradient updates
\end{itemize}

\subsection{Momentum}

On $i^{th}$ gradient descent, add momentum (second term)
	
\begin{eqn}
	\Delta_i w = \alpha \frac{\partial J}{\partial w} + \beta \Delta_{i-1}w
\end{eqn}

\begin{itemize}
	\item Allows us to pass small local minima 
	\item Keeps weights moving if error is flat
	\item However, can get out of a global maximum 
	\item Is tunable, increasing chance of divergence
\end{itemize}

\section{Convolutional Neural Network (CNN)}

\begin{itemize}
	\item Often used in computer vision
	\item Input usually 3D tensor for 2D RGB image
	\item Local receptive fields
	\begin{itemize}
		\item Hidden unit connected to patch of input image
		\item Unit connected to all 3 colours channels
	\end{itemize}
	\item Share matrix of parameters across units
	\begin{itemize}
		\item Units within depth slice at all positions have same weights 
		\item Feature map computed via discrete convolution with kernel matrix
	\end{itemize}
	\item Pooling/subsampling of hidden units in same neighbourhood
	\begin{itemize}
		\item Dimensionality reduced, making features more high-level \& abstract
	\end{itemize}
	\item Convolutional ``kernels'' aim to extract useful information (eg blurring/edge detection)
	\item Can have multiple layers, vary width/size of receptive fields, and vary stride (spacing between receptive fields)
	\item Pooling layers often added between convolutional layers; takes max of inputs in receptive fields
	\item Dropout regularization
	\begin{itemize}
		\item Helps generalize better \& reduce overfitting
		\item Independently set each hidden unit to zero with probability $p$ (often $p = 0.5$)
	\end{itemize}
	\item Batch normalization
	\begin{itemize}
		\item Compute mean \& variance independently for each dimension and normalize each input 
		\item Usually done already at input layer, but this extends it to other layers 
		\item Results in more stable gradient estimates
	\end{itemize}
	\item Can use softmax instead of sigmoid for multi-class classification (not the same as multi-output classification)
\end{itemize}

\subsection{Recurrent Neural Network (RNN)}

\begin{itemize}
	\item Often used in text \& speech
	\item Allows us to retain temporal information in sequences
	\item Add cycles with time delay (affects next step, to avoid infinite loops); see recurrence below [\ref{sec:recurrence}]
	\item Output types
	\begin{itemize}
		\item One at the end of the sequence (eg sentiment classification)
		\item One at each time step (eg language generation)
		\begin{itemize}
			\item Use softmax 
			\item Usually add an ``end of sentence'' token for stopping
		\end{itemize}
	\end{itemize}
	\item One-hot vectors
	\begin{itemize}
		\item Map each value to column of a weight matrix 
		\item Removes bias associated with categorical value
	\end{itemize}
	\item Train using backpropagation through time (BPTT)
	\begin{itemize}
		\item Same as regular backpropagation, but we unroll the computation graphto find the flow through all layers 
		\item Can truncate flow by using only last k time steps
	\end{itemize}
\end{itemize}

\subsection{Recurrence}
\label{sec:recurrence}

Elman RNN 

\begin{eqn}
	h_t = \sigma(Wh_{t-1} + Ux_t + b) \\ 
	o_t = \phi(Vh_t + c)
\end{eqn}

Jordan RNN

\begin{eqn}
	h_t = \sigma(Wo_{t-1} + Ux_t + b) \\ 
	o_t = \phi(Vh_t + c)
\end{eqn}

\begin{itemize}
	\item Elman usually better, as output is often constrained, resulting in information loss
\end{itemize}

\subsection{Long Short-Term Memory (LSTM)}

\begin{itemize}
	\item Long-term dependencies are hard to learn. Given that the transition matrix $W$ is the same for each step, a small offset from 1 will cause gradients to explode or vanish
	\item Can avoid exploding gradients through gradient clipping. If gradient magnitude is better than threshold, set it to $threshold * sign(gradient)$
	\item Can also use non-multiplicative interactions like LSTM (additive)
	\item LSTM has a hidden state (past info for next prediction) and cell state (past info for future predictions)
	\item Uses gates to control information flow 
	\item Input gate - what info from current input \& previous hidden state do we want to transfer to cell state?
	\begin{eqn}
		i_t &= \sigma(W_i \cdot \left[ h_{t-1}, x_t \right] + b_i) \\ 
		\tilde{C}_t &= tanh(W_C \cdot \left[ h_{t-1}, x_t \right] + b_C)
	\end{eqn}
	\item Cell update - additive linear combination of old cell state \& processed input 
	\begin{eqn}
		C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
	\end{eqn}
	\item Output gate - what information from cell state do we need for next prediction? 
	\begin{eqn}
		o_t &= \sigma(W_o \left[ h_{t-1}, x_t \right] + b_o) \\
		h_t &= o_t * tanh(C_t)
	\end{eqn}
	\item Use bidirectional LSTMs to increase expressiveness; apply both left-ro-right and right-to-left 
	\item Can also add attention by adding connections from encoder hidden states to context vector
\end{itemize}

\divider 

\begin{itemize}
	\item To produce output of a different length from input, use two RNNs. `Encoder' RNN transforms input to vector. `Decoder' RNN generates sequence from vector. Vector is often referred to as the ``context'' vector, and as used as input to the decoder at every timestep, along with the output at the previous timestep.
	\item Teacher forcing - train model by inputting ground-truth instead of input from previous iteration. No change done for testing as we don't have labels.
\end{itemize}

\end{document}   